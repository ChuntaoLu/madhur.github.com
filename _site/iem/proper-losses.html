<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>Probability Estimation: An Introduction &larr; Inductio Ex Machina</title>
   <meta name="author" content="Mark Reid" />

   <link rel='openid.server' href='http://www.myopenid.com/server' />
   <link rel='openid.delegate' href='http://mark.reid.name' />

   <link rel="start" href="/" />

	
	
	
  	<link rel="alternate" type="application/atom+xml" href="atom.xml" title="RSS feed" />
	

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="/files/css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="/files/css/screen.css" type="text/css" />

</head>
<body id="">
<div id="site">

  
<div id="header">
	<h1>
	<a href="/iem/" title="A machine learning blog">Inductio ex Machina</a>
	<span class="byline">&larr; <a href="/">Mark Reid</a></span>
</h1>
<ul class="nav">
  <li><a class="home" href="/iem/">Home</a></li>
  <li><a class="info" href="/iem/info.html">Info</a></li>
  <li><a class="past" href="/iem/past.html">Past</a></li>
  <li><a class="kith" href="/iem/kith.html">Kith</a></li>
</ul>

</div>

<div id="page">
	
  <h1 class="emphnext">Probability Estimation: An Introduction</h1>

<p>In my recent research I have been looking at relationships between various types of learning problems. One surprisingly rich class of problems is that of probability estimation. In this series of posts I will highlight some of the interesting theory I&#8217;ve uncovered about them in the machine learning, statistics and economics literature.</p>

<h2 id='binary_classification'>Binary Classification</h2>

<p>The quintessential type of learning problem in machine learning is binary classification. Given a training sample of instances, each labelled &#8220;positive&#8221; or &#8220;negative&#8221;, the aim is to learn to predict the correct label from previously unseen instances. A well known example of a binary classification problem is predicting whether an email is spam or not spam.</p>

<p>A binary classification problem can be stated as an optimisation: find function that minimises the average number of misclassifications on new instances drawn from the distribution that generated the training sample. Put another way, if we have to pay a penalty of $1 each time we predict a positive instance as negative or <em>vice versa</em> then we want to find a predictor that minimises our expected loss.<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup></p>

<p>Formally, we will say an instance <span class='maruku-inline'><img class='maruku-png' src='/images/latex/d7c817bcd91b45d2e83059cbc1fbfb32.png' alt='$x$' style='vertical-align: -0.0ex;height: 1.0ex;' /></span> is positive if it has associated label <span class='maruku-inline'><img class='maruku-png' src='/images/latex/b6b3c7c8a7d341752f70ca83759eefdd.png' alt='$y = 1$' style='vertical-align: -0.444444444444444ex;height: 2.0ex;' /></span> and negative if its label <span class='maruku-inline'><img class='maruku-png' src='/images/latex/57f17482e91b2599eef6e6e5431cf646.png' alt='$y = 0$' style='vertical-align: -0.444444444444444ex;height: 2.0ex;' /></span>. We then define the <em>0-1 misclassification loss</em> for a binary prediction <span class='maruku-inline'><img class='maruku-png' src='/images/latex/1673ac8aa9b48203da9b40ba52ef26de.png' alt='$p$' style='vertical-align: -0.444444444444444ex;height: 1.44444444444444ex;' /></span> when the label is <span class='maruku-inline'><img class='maruku-png' src='/images/latex/4d879bde964e549c6085ad630f56ad41.png' alt='$y$' style='vertical-align: -0.444444444444444ex;height: 1.44444444444444ex;' /></span> to be</p>
<div class='maruku-equation'><img class='maruku-png' src='/images/latex/009a2642e473266e56da4dc94c5838bc.png' alt='$\displaystyle	\ell_{01}(y,p) = 
	\begin{cases}
		1, &amp;	y \ne p \\
		0, &amp;	\text{otherwise}.
	\end{cases}$' style='height: 6.66666666666667ex;' /><span class='maruku-eq-tex'><code style='display: none'>\displaystyle	\ell_{01}(y,p) = 
	\begin{cases}
		1, &amp;	y \ne p \\
		0, &amp;	\text{otherwise}.
	\end{cases}</code></span></div>
<p>Now suppose that an instance <span class='maruku-inline'><img class='maruku-png' src='/images/latex/d7c817bcd91b45d2e83059cbc1fbfb32.png' alt='$x$' style='vertical-align: -0.0ex;height: 1.0ex;' /></span> has a positive label with probability <span class='maruku-inline'><img class='maruku-png' src='/images/latex/12e861a5f85f27422a1a536ee1247b43.png' alt='$\eta$' style='vertical-align: -0.555555555555556ex;height: 1.55555555555556ex;' /></span> and we have made a prediction <span class='maruku-inline'><img class='maruku-png' src='/images/latex/1673ac8aa9b48203da9b40ba52ef26de.png' alt='$p$' style='vertical-align: -0.444444444444444ex;height: 1.44444444444444ex;' /></span>. For that <span class='maruku-inline'><img class='maruku-png' src='/images/latex/d7c817bcd91b45d2e83059cbc1fbfb32.png' alt='$x$' style='vertical-align: -0.0ex;height: 1.0ex;' /></span> the <em>point-wise risk</em> is</p>
<div class='maruku-equation'><img class='maruku-png' src='/images/latex/b69ac48334edb38b980f2822e4065a0a.png' alt='$\displaystyle
	L_{01}(\eta,p) 
	= \eta\,\ell_{01}(1,p) + (1-\eta)\,\ell_{01}(0,p).$' style='height: 2.33333333333333ex;' /><span class='maruku-eq-tex'><code style='display: none'>\displaystyle
	L_{01}(\eta,p) 
	= \eta\,\ell_{01}(1,p) + (1-\eta)\,\ell_{01}(0,p).</code></span></div>
<p>The first term is the average loss of a prediction <span class='maruku-inline'><img class='maruku-png' src='/images/latex/1673ac8aa9b48203da9b40ba52ef26de.png' alt='$p$' style='vertical-align: -0.444444444444444ex;height: 1.44444444444444ex;' /></span> in the case of a positive example, occurring with probability <span class='maruku-inline'><img class='maruku-png' src='/images/latex/12e861a5f85f27422a1a536ee1247b43.png' alt='$\eta$' style='vertical-align: -0.555555555555556ex;height: 1.55555555555556ex;' /></span>, and the second term is the average loss for a negative example, occurring with probability <span class='maruku-inline'><img class='maruku-png' src='/images/latex/291e39803fba76cfc9a59fcee9cfac27.png' alt='$1-\eta$' style='vertical-align: -0.555555555555556ex;height: 2.11111111111111ex;' /></span>.</p>

<p>Returning to the spam example, suppose that with probability 0.95 a randomly chosen recipient says a particular email is spam. A prediction of &#8220;spam&#8221; for that email will incur an average loss of <span class='maruku-inline'><img class='maruku-png' src='/images/latex/e966d0bd2400ad87ef936ea3dd8c7741.png' alt='$0.95\times 0 + 0.05\times 1 = 0.05$' style='vertical-align: -0.222222222222222ex;height: 1.77777777777778ex;' /></span> whereas a prediction of &#8220;not spam&#8221; incurs a loss of 0.95.</p>

<h2 id='probability_estimation'>Probability Estimation</h2>

<p>Now suppose that instead of merely predicting the correct label we wanted to know the <em>probability</em> that an email was considered spam. In this case we would have a different but related type of learning problem: binary class probability estimation.</p>

<p>As predictions here are probabilities instead of concrete predictions, there is no sensible notion of a misclassification. How can a prediction that an email is spam with probability 0.9 be wrong? If it really isn&#8217;t spam it may just be one of the 10% of cases that are consistent with the probability estimate.</p>

<p>What we really want is a penalty with an <em>expected value</em> that is minimised if our probability estimates are consistent with the <em>true probability</em> of a positive label for a given instance. This fairly natural requirement on the loss for a probability estimation problem is known as <em>Fisher consistency</em>.</p>

<p>If <span class='maruku-inline'><img class='maruku-png' src='/images/latex/36588215fd5810c17501bb1214777fc8.png' alt='$\ell(y,p)$' style='vertical-align: -0.555555555555556ex;height: 2.33333333333333ex;' /></span> is a loss for probability estimation then the above requirement can be framed in terms of its associated point-wise risk: <span class='maruku-inline'><img class='maruku-png' src='/images/latex/592597c51b7a4aa2b4ead62743024112.png' alt='$L(\eta,p) = \eta\,\ell(1,p) + (1-\eta)\,\ell(0,p)$' style='vertical-align: -0.555555555555556ex;height: 2.33333333333333ex;' /></span>. Stated formally, Fisher consistency says that no matter what true probability <span class='maruku-inline'><img class='maruku-png' src='/images/latex/12e861a5f85f27422a1a536ee1247b43.png' alt='$\eta$' style='vertical-align: -0.555555555555556ex;height: 1.55555555555556ex;' /></span> we have</p>
<div class='maruku-equation'><img class='maruku-png' src='/images/latex/3f19c18ceb40b7e00183da3b2cf15015.png' alt='$\displaystyle	\min_{p\in[0,1]} L(\eta,p) = L(\eta,\eta).$' style='height: 3.66666666666667ex;' /><span class='maruku-eq-tex'><code style='display: none'>\displaystyle	\min_{p\in[0,1]} L(\eta,p) = L(\eta,\eta).</code></span></div>
<p>That is, predicting <span class='maruku-inline'><img class='maruku-png' src='/images/latex/85c94d02a95009b5fd9410bf973ab842.png' alt='$p = \eta$' style='vertical-align: -0.555555555555556ex;height: 1.55555555555556ex;' /></span> always achieves the smallest possible point-wise risk.</p>

<p>We will call losses that have this Fisher consistency property <em>proper losses</em> in line with the terminology of <em>proper scoring rules</em> used when probability elicitation is studied in economics.<sup id='fnref:2'><a href='#fn:2' rel='footnote'>2</a></sup> We will see several interesting connections between these two concepts in future posts.</p>

<h2 id='examples'>Examples</h2>

<p>One common loss functions used for probability estimation is <em>square loss</em></p>
<div class='maruku-equation'><img class='maruku-png' src='/images/latex/83f74bea05c65573dda9adf4305b564a.png' alt='$\displaystyle
	\ell_{\text{sq}}(y,p) = y(1-p)^2 + (1-y) p^2.$' style='height: 2.55555555555556ex;' /><span class='maruku-eq-tex'><code style='display: none'>\displaystyle
	\ell_{\text{sq}}(y,p) = y(1-p)^2 + (1-y) p^2.</code></span></div>
<p>The easiest way to convince yourself this is Fisher consistent is to examine when the derivatives of its point-wise risk with respect to <span class='maruku-inline'><img class='maruku-png' src='/images/latex/1673ac8aa9b48203da9b40ba52ef26de.png' alt='$p$' style='vertical-align: -0.444444444444444ex;height: 1.44444444444444ex;' /></span> vanishes. In the case of square loss we see that</p>
<div class='maruku-equation'><img class='maruku-png' src='/images/latex/393c4ba7d359c057e09ed82a89884954.png' alt='$\frac{\partial}{\partial p} L_{\text{sq}}(\eta,p)
	= -2\eta(1-p) + 2(1-\eta)p = 2(p-\eta)$' style='height: 3.11111111111111ex;' /><span class='maruku-eq-tex'><code style='display: none'>\frac{\partial}{\partial p} L_{\text{sq}}(\eta,p)
	= -2\eta(1-p) + 2(1-\eta)p = 2(p-\eta)</code></span></div>
<p>which is 0 only when <span class='maruku-inline'><img class='maruku-png' src='/images/latex/85c94d02a95009b5fd9410bf973ab842.png' alt='$p=\eta$' style='vertical-align: -0.555555555555556ex;height: 1.55555555555556ex;' /></span> and so <span class='maruku-inline'><img class='maruku-png' src='/images/latex/c98af96fe8ba2731158202ed577f8deb.png' alt='$\ell_{\text{sq}}$' style='vertical-align: -0.666666666666667ex;height: 2.22222222222222ex;' /></span> is proper.</p>

<p>While Fisher consistency seems like a fairly innocuous and natural constraint for probability estimation it has some impressive implications that I will explore in some future posts.</p>
<div class='footnotes'><hr /><ol><li id='fn:1'>
<p>Of course, in the case of spam the penalty is not as symmetric as described here. Incorrectly predicting spam as &#8220;not spam&#8221; is mildly annoying whereas predicting that an important email that your career depends upon as &#8220;spam&#8221; and sending it to the Trash folder is potentially disastrous!</p>
<a href='#fnref:1' rev='footnote'>&#8617;</a></li><li id='fn:2'>
<p>Scoring rules are usually framed in terms of rewards rather than penalties so loss and scoring rules are additive inverse and minimisation here becomes maximisation there.</p>
<a href='#fnref:2' rev='footnote'>&#8617;</a></li></ol></div>

  <address class="signature">
    <a class="author" href="http://mark.reid.name">Mark Reid</a> 
    <span class="date">01 March 2009</span>
    <span class="location">Canberra, Australia</span>
  </address>
</div><!-- End Page -->

<!-- Delicious hits
<script type="text/javascript">
    if (typeof window.Delicious == "undefined") window.Delicious = {};
    Delicious.BLOGBADGE_DEFAULT_CLASS = 'delicious-blogbadge-line';
</script>
<script src="http://static.delicious.com/js/blogbadge.js"></script>
-->


<!-- Discus Comments -->
<div id="disqus_thread"></div>

<!-- Enable Disqus comments -->
<script type="text/javascript">
	var disqus_iframe_css = "http://mark.reid.name/css/screen.css";
	var disqus_title = "Probability Estimation: An Introduction";
	var disqus_message = "Probability estimation is an important class of problem in machine learning. In this, the first of a series of posts, I discuss a natural class of losses for these problems.";
</script>
<script type="text/javascript" src="http://disqus.com/forums/markreid/embed.js"></script>

<noscript>
		<a href="http://markreid.disqus.com/?url=ref">View the discussion thread.</a>
</noscript>

  
  <div id="footer">
	<address>
		<span class="copyright">
			Content &amp; Design by 
			<a href="/info/site.html">Mark Reid</a>
			<br/>
			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
		</span>
		<span class="engine">
			Powered by 
			<a href="http://github.com/mreid/jekyll/" title="A static, minimalist CMS">Jekyll</a>
		</span>
	</address>
  </div>
</div>

<!-- Google Analytics script goes here -->
<script type="text/javascript" src="http://twitter.com/javascripts/blogger.js"></script>
<script type="text/javascript" src="http://twitter.com/statuses/user_timeline/mdreid.json?callback=t
witterCallback2&count=1"></script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-1051817-4");
pageTracker._trackPageview();
</script>
<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->
</body>
</html>
