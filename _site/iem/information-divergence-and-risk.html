<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>Information, Divergence and Risk for Binary Experiments &larr; Inductio Ex Machina</title>
   <meta name="author" content="Madhur Ahuja" />

     <link rel="start" href="/" />

	
	
	
  	<link rel="alternate" type="application/atom+xml" href="atom.xml" title="RSS feed" />
	

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="/files/css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="/files/css/screen.css" type="text/css" />

</head>
<body id="">
<div id="site">

  
<div id="header">
	<h1>
	<a href="/iem/" title="A machine learning blog">Inductio ex Machina</a>
	<span class="byline">&larr; <a href="/">Mark Reid</a></span>
</h1>
<ul class="nav">
  <li><a class="home" href="/iem/">Home</a></li>
  <li><a class="info" href="/iem/info.html">Info</a></li>
  <li><a class="past" href="/iem/past.html">Past</a></li>
  <li><a class="kith" href="/iem/kith.html">Kith</a></li>
</ul>

</div>

<div id="page">
	
  <h1 class="emphnext">Information, Divergence and Risk for Binary Experiments</h1>

<p><a href='http://axiom.anu.edu.au/~williams/'>Bob Williamson</a> and I have finished a <a href='http://arxiv.org/abs/0901.0356'>report</a> outlining what we have been looking at for the last year or so and uploaded it to the arXiv. Weighing in at 89 pages, it covers a lot of ground in an attempt to unify a number of different classes of measures for problems that can be expressed as binary experiments. That is, where instances are drawn from two distributions. This include binary classification, class probability estimation, and hypothesis testing.</p>

<p>We show that many of the usual measures of difficultly for these problems — divergence, information and Bayes risk — are very closely related. We also look at ways in which members of each class of measure can be expressed in terms of &#8220;primitive&#8221; members of those classes. In particular, Fisher-consistent losses (also known as proper scoring rules) can be written as weighted sums of cost-sensitive loss while all f-divergences can be written as weighted sums of something akin to cost-sensitive variational divergence. These &#8220;Choquet representations&#8221; make it easy to derive Pinsker-like bounds for arbitrary f-divergences (not just KL divergence) as well as results similar to those of Bartlett et al in their &#8221;<a href='http://www.citeulike.org/user/mdreid/article/510440'>Convexity, classification and Risk Bounds</a>&#8221;.</p>

<p>It should be made clear that many of these results are not new. However, what I like about our approach is that almost all of the results in the paper stem from a two observations about convex functions: they are invariant under the Legendre-Fenchel bidual, and they have a second-order integral Taylor expansion with non-negative weights.</p>

<p>If any of this sounds interesting, you should grab the full paper from the <a href='http://arxiv.org/abs/0901.0356'>arXiv</a>. Here&#8217;s the abstract:</p>

<blockquote>
<p>We unify f-divergences, Bregman divergences, surrogate loss bounds (regret bounds), proper scoring rules, matching losses, cost curves, ROC-curves and information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their primitives which all are related to cost-sensitive binary classification. As well as clarifying relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate loss bounds and generalised Pinsker inequalities relating f-divergences to variational divergence. The new viewpoint illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates Maximum Mean Discrepancy to Fisher Linear Discriminants. It also suggests new techniques for estimating f-divergences.</p>
</blockquote>

<p>Now that we have a good understanding of binary experiments the aim is to build on these results and extend this type of work to other forms of machine learning problems. High on the list are multi-category classification, ranking and regression problems.</p>

<p>Questions, criticism, suggestions and pointers to related work we may have missed are all welcome.</p>

  <address class="signature">
    <a class="author" href="http://madhur.github.com">Madhur Ahuja</a> 
    <span class="date">06 January 2009</span>
    <span class="location">Canberra, Australia</span>
  </address>
</div><!-- End Page -->


<!-- Discus Comments -->
<div id="disqus_thread"></div>

<!-- Enable Disqus comments -->
<script type="text/javascript">
	var disqus_iframe_css = "http://mark.reid.name/css/screen.css";
	var disqus_title = "Information, Divergence and Risk for Binary Experiments";
	var disqus_message = "A summary of a recent paper Bob and I posted to arXiv.";
</script>
<script type="text/javascript" src="http://disqus.com/forums/markreid/embed.js"></script>

<noscript>
		<a href="http://markreid.disqus.com/?url=ref">View the discussion thread.</a>
</noscript>

  
  <div id="footer">
	<address>
		<span class="copyright">
			Content &amp; Design by 
			<a href="/info/site.html">Madhur Ahuja</a>
			<br/>
			(Some rights reserved)			
		</span>
		<span class="engine">
			Powered by 
			<a href="http://github.com/mreid/jekyll/" title="A static, minimalist CMS">Jekyll</a>
		</span>
	</address>
  </div>
</div>
<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->
</body>
</html>
